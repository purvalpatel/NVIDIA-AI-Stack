
## Below are some best inference runtimes:

If you want **raw speed** ‚Üí **TensorRT-LLM**. <br>
If you want **easy, stable APIs** ‚Üí **vLLM or NIM**. <br>
If you want **scalable production** ‚Üí **Triton**. <br>
If you want **local/offline** ‚Üí **llama.cpp** / **Ollama**. <br>


Below is the typical architecture in terms of working with Nvidia AI Stack:
---------------------------------------------------
Training (NeMo Framework) -> Optimization (TensorRT) -> serving (Triton/NIM)  <br>


### Big Picture:

| Layer | Tool / Framework | Mainly Used By | Purpose |
|-------|------------------|----------------|----------|
| üß© **Model Development** | **NeMo Framework** | Data Scientists / ML Researchers | Build, train, and fine-tune AI models (LLMs, ASR, etc.) |
| üöÄ **Model Packaging & Serving** | **NIM (NVIDIA Inference Microservices)** | MLOps Engineers / AI Engineers | Deploy models as scalable microservices (APIs) |
| ‚öôÔ∏è **Inference Optimization** | **TensorRT / TensorRT-LLM** | MLOps / System Engineers | Optimize model performance for fast GPU inference |
| üñ•Ô∏è **Serving Infrastructure** | **Triton Inference Server** | MLOps / DevOps Engineers | Host and serve multiple models efficiently |
| üß∞ **Monitoring / Scaling** | **Kubernetes, Helm, ArgoCD** | MLOps / Platform Engineers | Manage and scale model deployments |


There are 5 main types of LLM model files you will see:
---------------
| Format                  | File Example                              | What it is                      | Where It Comes From         | Can Run On                                                                               |
| ----------------------- | ----------------------------------------- | ------------------------------- | --------------------------- | ---------------------------------------------------------------------------------------- |
| **PyTorch Checkpoints** | `pytorch_model.bin` / `model.safetensors` | Raw weights                     | Hugging Face, Meta releases | **vLLM**, **PyTorch**, **Transformers**, **Triton**, **TensorRT-LLM (after conversion)** |
| **GGUF**                | `model.Q4_K_M.gguf`                       | Quantized CPU/GPU format        | llama.cpp community         | **llama.cpp**, **Ollama**, **koboldcpp**, **LM Studio**                                  |
| **ONNX**                | `model.onnx`                              | Framework-agnostic graph format | Export tools, ONNX Runtime  | **ONNX Runtime**, **Triton Inference Server**                                            |
| **TensorRT Engines**    | `model.plan`                              | Optimized GPU execution engine  | **TensorRT-LLM build** step | **TensorRT Runtime**, **NIM**, **Triton Server**, **trtllm-infer**                       |
| **safetensors**         | `model-0001-of-0002.safetensors`          | Safe, memory-mapped HF weights  | Hugging Face                | Same as PyTorch: **vLLM**, **Transformers**, etc.                                        |


### Which runtime supports which format?

| Runtime                     | Supports Safetensors | Supports GGUF |     Supports ONNX     | Supports TensorRT Engine | Notes                          |
| --------------------------- | :------------------: | :-----------: | :-------------------: | :----------------------: | ------------------------------ |
| **vLLM**                    |         ‚úÖ Yes        |      ‚ùå No     |       ‚ùå Limited       |           ‚ùå No           | Best for fast server inference |
| **PyTorch / Transformers**  |         ‚úÖ Yes        |      ‚ùå No     | ‚úÖ Yes (via exporters) |           ‚ùå No           | Training + flexible inference  |
| **TensorRT-LLM**            |   ‚ö†Ô∏è Needs convert   |      ‚ùå No     |          ‚ùå No         |           ‚úÖ Yes          | Requires **conversion step**   |
| **NVIDIA NIM**              |   ‚úÖ (auto convert)   |      ‚ùå No     |  ‚úÖ Yes (some models)  |           ‚úÖ Yes          | Production-grade API server    |
| **Triton Inference Server** |         ‚úÖ Yes        |      ‚ùå No     |         ‚úÖ Yes         |           ‚úÖ Yes          | Enterprise serving platform    |
| **Ollama**                  |         ‚ùå No         |     ‚úÖ Yes     |          ‚ùå No         |           ‚ùå No           | Simple local inference         |
| **llama.cpp**               |         ‚ùå No         |     ‚úÖ Yes     |          ‚ùå No         |           ‚ùå No           | CPU or small GPU inference     |

### When to use which format?

| Goal                                             | Best Format               | Best Runtime                  |
| ------------------------------------------------ | ------------------------- | ----------------------------- |
| **High performance GPU inference (H100 / A100)** | **TensorRT Engine**       | **NIM, Triton, TensorRT-LLM** |
| **Fast inference on consumer GPUs (3090/4090)**  | **vLLM + safetensors**    | **vLLM**                      |
| **Run on Mac / CPU / small GPU**                 | **GGUF**                  | **Ollama / llama.cpp**        |
| **Fine-tune or train**                           | **PyTorch / safetensors** | **PyTorch / Transformers**    |


Real Flow of deploying model:
--------------------------
When we download a model from Hugging Face, it usually comes in:
```
safetensors / .bin weights + config + tokenizer
```

| Runtime / Serving System               | Model Format Required          | Conversion Needed?                  | Notes                        |
| -------------------------------------- | ------------------------------ | ----------------------------------- | ---------------------------- |
| **PyTorch / HuggingFace Transformers** | safetensors / .bin             | ‚ùå No                                | Slowest but simplest         |
| **vLLM**                               | safetensors / .bin (HF format) | ‚ùå No                                | Efficient, fast, easy        |
| **ONNX Runtime**                       | .onnx                          | ‚úÖ Convert ‚Üí ONNX                    | Usually CPU or GPU inference |
| **TensorRT-LLM**                       | `.plan` Engine                 | ‚úÖ Convert ‚Üí TRT checkpoint ‚Üí Engine | Fastest on GPU (H100 / A100) |


### If using vLLM
```
HuggingFace model ‚Üí serve directly
```

### If using ONNX
```
HuggingFace model ‚Üí convert to ONNX ‚Üí serve ONNX
```

### If using TensorRT-LLM
```
HuggingFace model (.safetensors) 
        ‚Üì convert_checkpoint.py
TensorRT-LLM checkpoint
        ‚Üì trtllm-build
TensorRT Engine (.plan)
        ‚Üì trtllm-infer / trtllm-serve / NIM / Triton
```

### Which path should we use ?
| Hardware                                    | Recommended Runtime    |
| ------------------------------------------- | ---------------------- |
| **H100 / A100 GPU server (enterprise)**     | **TensorRT-LLM / NIM** |
| **Single GPU consumer cards (4090 / 4080)** | **vLLM**               |
| **CPU only**                                | **ONNX Runtime**       |
| **Laptop / mobile**                         | **GGUF + llama.cpp**   |

