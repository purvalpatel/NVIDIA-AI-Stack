
## Below are some best inference runtimes:

If you want **raw speed** ‚Üí **TensorRT-LLM**. <br>
If you want **easy, stable APIs** ‚Üí **vLLM or NIM**. <br>
If you want **scalable production** ‚Üí **Triton**. <br>
If you want **local/offline** ‚Üí **llama.cpp** / **Ollama**. <br>


#### In sort: <br>

Training (NeMo Framework) -> Optimization (TensorRT) -> serving (Triton/NIM)  <br>


Big Picture:
----------
| Layer | Tool / Framework | Mainly Used By | Purpose |
|-------|------------------|----------------|----------|
| üß© **Model Development** | **NeMo Framework** | Data Scientists / ML Researchers | Build, train, and fine-tune AI models (LLMs, ASR, etc.) |
| üöÄ **Model Packaging & Serving** | **NIM (NVIDIA Inference Microservices)** | MLOps Engineers / AI Engineers | Deploy models as scalable microservices (APIs) |
| ‚öôÔ∏è **Inference Optimization** | **TensorRT / TensorRT-LLM** | MLOps / System Engineers | Optimize model performance for fast GPU inference |
| üñ•Ô∏è **Serving Infrastructure** | **Triton Inference Server** | MLOps / DevOps Engineers | Host and serve multiple models efficiently |
| üß∞ **Monitoring / Scaling** | **Kubernetes, Helm, ArgoCD** | MLOps / Platform Engineers | Manage and scale model deployments |
